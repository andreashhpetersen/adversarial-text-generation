\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,bm}
\usepackage{mathtools}
\usepackage{graphicx,caption}
\usepackage{listings}
\usepackage[margin=1.2in]{geometry}

\usepackage{titling}
\usepackage{lipsum}

\usepackage{parskip}

\usepackage{url}

\usepackage[
    style=authoryear-icomp,
    maxbibnames=9,
    maxcitenames=2,
    backend=biber
]{biblatex}
\addbibresource{sample.bib}

\setlength{\jot}{10pt}

\allowdisplaybreaks[1]

% Expectation symbol
\DeclareMathOperator*{\E}{\mathbb{E}}

% Math functions
\DeclareMathOperator{\Span}{span}

% Conditional
\newcommand\given[1][]{\:#1\vert\:}

% argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\softmax}{soft\,max}

\graphicspath{{images/}}

\title{%
    Adversarial Text Generation\\
    \large NLP and Deep Learning --- Final Project
}

\author{%
    Andreas Holck HÃ¸eg-Petersen\\
    \texttt{anhh@itu.dk}
    \and
    Mathias Bastholm\\
    \texttt{mbas@itu.dk}
}

\begin{document}
\maketitle

\begin{abstract}
    \lipsum[1]
\end{abstract}

\section{Introduction}\label{sec:introduction}

In recent years, Generative Adversarial Networks (GANs) have gained a lot of
traction in the Deep Learning community because of their impressive results in
image generation. The general idea is that a generator and a discriminator are
jointly trained to produce an image output that is seemingly indistinguishable
from non-generated images. This model were first described
in~\cite{Goodfellow2014GenerativeAN}.

We want to attempt to apply this strategy for text generation. The main
difficulty for this task is that whereas image outputs can be considered a
continuous value, a sentence is inherently discrete as it is a sequence of words
each of which is chosen by the model using the non-differentiable $argmax$
function. To remedy this, we propose a model where the discriminator is trained
to distinguish between the continuous outputs of a pre-trained encoder given a
`true' sentence from the generated, `fake' output stemming from our generator.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{projectModel.png}
    \caption{% 
        Overview of the model architecture. The dotted lines from the
        $\mathbf{X}$s represents that the encoded and generated $\mathbf{X}$s
        will be fed to the discriminator and the decoder during training and
        evaluation, respectively.
    }\label{fig:projectModel}
\end{figure}

In our project, we will construct and train an autoencoder model that can encode
and decode a sentence from English to English. The encoded sentences are then
used as labelled training data for the discriminator, representing `true'
values. The job of the generator is to produce similar encodings but doing this
from random noise in a way that makes the discriminator unable to distinguish
between the encodings stemming from the autoencoder and the encodings stemming
from the generator.

Ideally, this would train the generator to produce sentence encodings that can
be fed to the decoder of the Transformer model which would then produce
meaningful sentences from this artificially generated input. See
Figure~\ref{fig:projectModel} for an overview of the complete model.

This project thus have two objectives: one is to construct a working autoencoder
that can map an English sentence to some hidden state $\mathbf{X}$ with a
corresponding decoder that can extract the original sentence from $\mathbf{X}$.
For convenience, we will refer to the encoder part of this model as the
`Teacher'. The second objective is to build a GAN network, where a generator ---
the `Student' --- must learn to produce approximations of $\mathbf{X}$.

The second objective is highly experimental as explained in
Section~\ref{sec:background}, where we will also describe other approaches at
using the GAN architecture for NLP problems. In Section~\ref{sec:method} we will
describe how we have build the different parts of the model and how we utilize
our dataset. Then in Section~\ref{sec:analysis} we will present our results and
discuss the shortcomings of the model\(s\), and in
Section~\ref{sec:furtherResearch} we will proceed to suggest improvements and
ideas for further research. Lastly, in Section~\ref{sec:conclusion} we conclude
on our project.


\section{Background}\label{sec:background}

Applications GANs have mainly focused on image generation and has not yet seen a
major breakthrough in text generation. As mentioned in
Section~\ref{sec:introduction}, this is because the discrete nature of text,
which is basically a sequence of words, requires a non-differentiable $argmax$
function to transform a probability distribution over a vocabulary to a single
value (ie.\ the word with the highest probability). This is depicted in
Figure~\ref{fig:argmaxGAN}.

\begin{figure}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \includegraphics[width=0.8\textwidth]{argmaxGAN.png}
    \caption{%
        A simple GAN model where the generator output is run through an
        $argmax$ function before being given to the discriminator. This prevents
        gradients to flow from the discriminator to the generator.
        Source:~\cite{haidar2019textkdgan}.
    }\label{fig:argmaxGAN}
\end{figure}

There have been, however, multiple attempts at working around this issue.
According to~\cite{Chintapalli2019} these approaches can broadly be categorized
into three types:

\begin{itemize}
    \item Reinforcement Learning-based solutions
    \item The Gumbel-Softmax approximations
    \item Avoiding discrete spaces by working with the continuous output of the
        generaor
\end{itemize}

In this project, we follow the third approach, but in this section we will give
short introductions to the idea behind the two first.

As an example of an RL-based solution,~\cite{yu2016seqgan} proposes the SeqGAN
model, in which the generator is considered an RL-agent with states
$\mathbf{s}_t$ being the text generated at timestep $t$ and actions $\mathbf{a}$
being all the possible words to choose next. The agent then chooses its next
word (takes an action $a$) based on some policy function $\bm{\pi}(a \given
\bm{s}_t, \bm{\theta})$, where $\bm{\theta}$ are the parameters to be
optimized. Using Monte-Carlo rollouts to produce a number of different
sentences sharing a prefix $\bm{s}_t$, the discriminator then rewards each
sentence and the averaged reward is then used to perform gradient ascent on
$\bm{J}(\bm{\theta})$, where $\bm{J}$ is the performance measure of
$\bm{\theta}$. This approach alleviates some of the problems of training a GAN
for text generation, but it suffers from an unstable and slow training process,
convergence to sub-optimal local minima and and extremely large state-space.\

Another approach is to use the Gumbel-Softmax distribution to approximate a
one-hot encoding of a probability distribution passed through the $argmax$
function. This is the approach taken by~\cite{kusner2016gans}. Here, a
$d$-dimensional one-hot encoding vector $\bm{y}$ is approximated using

\begin{equation}
    \bm{y} = \softmax(\frac{1}{\tau}(\bm{h}+\bm{g}))
\end{equation}

where $\bm{h}$ is some hidden state (ie.\ of an RNN), $\bm{g}$ is drawn from a
Gumbel distribution and $\tau$ is a temperature parameter. This works because it
is differentiable and as $\tau \to 0$ the distribution of $\bm{y}$ will match
that we get from

\begin{equation}
    \bm{y} = \text{one\_hot}(\argmax_{i}(h_i + g_i))
\end{equation}

which again can be shown to be the same as sample $\bm{y}$ from a probability
distribution $\bm{p} = softmax(\bm{h})$ where $p_i = p(y_i=1), i = 1\dots d$.

Finally, for other examples on the third approach,
see~\cite{donahue2018adversarial} and~\cite{haidar2019textkdgan}.


\section{Method}\label{sec:method}

For all our models (autoencoders and GANs), we drew inspiration PyTorch
tutorials
(\cite{pytorchTutorialAtt},~\cite{pytorchTutorialTransformer},~\cite{pytorchTutorialGAN}),
tweaking them to our specific needs. The whole process (project development,
research, data collection, coding, training, experimentation and analysis) was
conducted during a 10-day period.

This section will describe this process, focusing on the final outcomes rather
than including all our intermediate steps and missteps.

\subsection{Dataset}\label{sec:dataset}

For training of the autoencoders, we simply needed dataset consisting of a large
number of English sentences. We obtained this from Universal Dependencies, where
used the `Universal Dependencies --- English Dependency Treebank Universal
Dependencies English Web Treebank v2.6 --- 2020--05--15' (\cite{silveira14gold})
consisting of 12,543 training sentences, 2,077 test sentences, 2,002 dev
sentences and a vocabulary of 16,654 training tokens. The data is annotated with
metadata such as lemmas and word classes, but we discarded this information as
it was not relevant for our purpose.

Furthermore, we also utilized another dataset intended for training
English-to-French translation. This dataset originates from
\url{https://tatoeba.org/eng/} and consists of 135,842 sentence pairs. We
discarded the French sentences and removed all duplicate sentences and sentences
of length smaller than 3, as well as splitting all sentences that contained
punctuations, questionmarks, exclamation points, etc. This gave us a set of
92,343 sentences, which we split 80/20 between training and testing/development,
and a vocabulary of 13,731 tokens.


\subsection{Models}\label{sec:models}

We developed two different versions of the autoencoder model and also two
different GANs. These will be described in this subsection.

\subsubsection{The TransformerModel}\label{sec:transformermodel}

Our first autoencoder was based on~\cite{pytorchTutorialTransformer}. This model
consists of a very simple decoder, that is simply a feed-forward neural net that
takes a 2-dimensional tensor $X \in \mathbb{R}^{n \times k}$ and maps each of
the $n$ $k$-dimensional vectors of the sequence to a probability distribution
over the entire vocabulary which it then can convert to an output sequence using
$argmax$.

The encoder, however, is responsible for generating $X$ and it does so by using
the Transformer architecture as suggested in~\cite{vaswani2017attention} and
implemented in the PyTorch module \texttt{nn.Transformer}. For our purpose,
however, we only used the submodule \texttt{nn.TransformerEncoder}, which
consists of a stack of encoder layers that uses self-attention to focus on
specific, relevant parts of the input sequence in one go, and then passes its
output on to the next layer through a feed-forward network. As a preprocessing
step, before the input sequence is passed through the transformer, positional
encoding is added, as suggested in the paper (\cite{vaswani2017attention}).


\subsubsection{RNN and Attention based Autoencoder}\label{sec:attnRNN}


\subsubsection{Simple GAN}\label{sec:simpleGAN}


\subsubsection{Transformer GAN}\label{sec:simpleGAN}


\subsection{Training}\label{sec:training}


\section{Analysis}\label{sec:analysis}


\subsection{Results}\label{sec:results}


\subsection{Discussion}\label{sec:discussion}


\section{Further research}\label{sec:furtherResearch}


\section{Conclusion}\label{sec:conclusion}


\printbibliography%

\end{document}

